{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR='/home/thanuja/Dropbox/coursera/Milestone1/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from itertools import chain\n",
    "from pyspark.sql import types as t\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType,StructField, StringType,IntegerType\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark intitialization\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .appName('cms_physicians_analysis') \\\n",
    "    .getOrCreate() \n",
    "# accessing spark context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Taxonomies provided by CMS\n",
    "hcp_taxonomies =spark.read.options(header='True').csv(BASE_DIR + \"Medicare_Provider_and_Supplier_Taxonomy_Crosswalk_October_2021.csv\")\n",
    "#rename columns and remove whitespaces as appropriate.\n",
    "hcp_taxonomies = hcp_taxonomies.withColumnRenamed(\"PROVIDER TAXONOMY DESCRIPTION:  TYPE, CLASSIFICATION, SPECIALIZATION\",\"detail_desc\")\\\n",
    "                               .withColumnRenamed(\"MEDICARE PROVIDER/SUPPLIER TYPE DESCRIPTION\",\"hl_desc\")\\\n",
    "                               .withColumnRenamed(\"MEDICARE SPECIALTY CODE\",\"sp_code\")\\\n",
    "                               .withColumnRenamed(\"PROVIDER TAXONOMY CODE\",\"tx_code\")\n",
    "hcp_taxonomies = hcp_taxonomies.withColumn('tx_code', trim(hcp_taxonomies.tx_code))\n",
    "hcp_taxonomies.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionaries with taxcodes as key and value being high level and detailed descriptions of specialities.\n",
    "tx_codes = hcp_taxonomies.select(F.collect_list('tx_code')).first()[0]\n",
    "detail_descs = hcp_taxonomies.select(F.collect_list('detail_desc')).first()[0]\n",
    "tax_detail_dict = dict(zip(tx_codes, detail_descs))\n",
    "\n",
    "hl_descs = hcp_taxonomies.select(F.collect_list('hl_desc')).first()[0]\n",
    "tax_hl_dict = dict(zip(tx_codes, hl_descs))\n",
    "\n",
    "# this file has mapping of specialities between CMS taxonomy file and DAC_NationalDownloadableFile.csv\n",
    "cms_npi_map = pd.read_csv(BASE_DIR + \"mapping_taxonomies.csv\")\n",
    "\n",
    "# dictionary of the mapping cms speciality and specialities listed in DAC_NationalDownloadableFile.csv\n",
    "cms_nat_dict = dict(zip(cms_npi_map.CMS_SPECIALITY, cms_npi_map.NAT_SPECIALITY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#physcians supplemental file (fname,lname,mname,address,state,city,zip,taxonomy,speciality)\n",
    "hcp_suppl_file_df = spark.read.options(header='True').csv(BASE_DIR + \"Physician Supplement File for all Program Years/OP_PH_PRFL_SPLMTL_P06302021.csv\")\n",
    "\n",
    "hcp_suppl_file_df = hcp_suppl_file_df.withColumnRenamed(\"Physician_Profile_ID\", \"ppid\")\\\n",
    "                                           .withColumnRenamed(\"Physician_Profile_First_Name\",\"f_name\")\\\n",
    "                                           .withColumnRenamed(\"Physician_Profile_Last_Name\",\"l_name\")\\\n",
    "                                           .withColumnRenamed(\"Physician_Profile_Alternate_Middle_Name\",\"ma_name\")\\\n",
    "                                           .withColumnRenamed(\"Physician_Profile_Middle_Name\",\"m_name\")\\\n",
    "                                           .withColumnRenamed(\"Physician_Profile_Address_Line_1\",\"adr_ln_1\")\\\n",
    "                                           .withColumnRenamed(\"Physician_Profile_Address_Line_2\",\"adr_ln_2\")\\\n",
    "                                           .withColumnRenamed(\"Physician_Profile_City\",\"city\")\\\n",
    "                                           .withColumnRenamed(\"Physician_Profile_State\",\"state\")\\\n",
    "                                           .withColumnRenamed(\"Physician_Profile_Zipcode\",\"zip\")\\\n",
    "                                           .withColumnRenamed(\"Physician_Profile_OPS_Taxonomy_1\",\"txcode_1\")\\\n",
    "                                           .withColumnRenamed(\"Physician_Profile_OPS_Taxonomy_2\",\"txcode_2\")\\\n",
    "                                           .withColumnRenamed(\"Physician_Profile_OPS_Taxonomy_3\",\"txcode_3\")\n",
    "\n",
    "\n",
    "hcp_suppl_file_df = hcp_suppl_file_df.withColumn('txcode_1', trim(hcp_suppl_file_df.txcode_1))\n",
    "\n",
    "#separate 5 digit zip from 4 Codes\n",
    "hcp_suppl_file_df = hcp_suppl_file_df.withColumn(\"zip\", F.regexp_replace(\"zip\", \"-\", \" \"))\n",
    "\n",
    "#add specialities from DAC_NationalDownloadableFile (NAT) to supplemental file for 3 different taxonomy codes.\n",
    "mapping_expr1 = create_map([lit(x) for x in chain(*tax_hl_dict.items())])\n",
    "mapping_expr2 = create_map([lit(x) for x in chain(*tax_detail_dict.items())])\n",
    "mapping_expr3 = create_map([lit(x) for x in chain(*cms_nat_dict.items())])\n",
    "hcp_suppl_file_df = hcp_suppl_file_df.withColumn(\"cms_hl_speciality_1\", mapping_expr1.getItem(col(\"txcode_1\")))\n",
    "hcp_suppl_file_df = hcp_suppl_file_df.withColumn(\"cms_detail_speciality_1\", mapping_expr2.getItem(col(\"txcode_1\")))\n",
    "hcp_suppl_file_df = hcp_suppl_file_df.withColumn(\"nat_speciality_1\", mapping_expr3.getItem(col(\"txcode_1\")))\n",
    "\n",
    "hcp_suppl_file_df = hcp_suppl_file_df.withColumn(\"cms_hl_speciality_2\", mapping_expr1.getItem(col(\"txcode_2\")))\n",
    "hcp_suppl_file_df = hcp_suppl_file_df.withColumn(\"cms_detail_speciality_2\", mapping_expr2.getItem(col(\"txcode_2\")))\n",
    "hcp_suppl_file_df = hcp_suppl_file_df.withColumn(\"nat_speciality_2\", mapping_expr3.getItem(col(\"txcode_2\")))\n",
    "\n",
    "hcp_suppl_file_df = hcp_suppl_file_df.withColumn(\"cms_hl_speciality_3\", mapping_expr1.getItem(col(\"txcode_3\")))\n",
    "hcp_suppl_file_df = hcp_suppl_file_df.withColumn(\"cms_detail_speciality_3\", mapping_expr2.getItem(col(\"txcode_3\")))\n",
    "hcp_suppl_file_df = hcp_suppl_file_df.withColumn(\"nat_speciality_3\", mapping_expr3.getItem(col(\"txcode_3\")))\n",
    "\n",
    "\n",
    "#concat columns and get unique values for each column combinations.\n",
    "\n",
    "hcp_suppl_file_df = hcp_suppl_file_df.withColumn(\n",
    "    \"state_city\", concat_ws(\" \", hcp_suppl_file_df['state'],\n",
    "                              hcp_suppl_file_df['city'],\n",
    "                              hcp_suppl_file_df['zip']))\n",
    "hcp_suppl_file_df = hcp_suppl_file_df.withColumn(\n",
    "    \"address\", concat_ws(\" \", hcp_suppl_file_df['adr_ln_1'],\n",
    "                              hcp_suppl_file_df['adr_ln_2']))\n",
    "hcp_suppl_file_df = hcp_suppl_file_df.withColumn(\n",
    "    \"taxonomies\", F.concat_ws(' ',\n",
    "                F.array_distinct(F.split(concat_ws(\" \", hcp_suppl_file_df['txcode_1'],\n",
    "                              hcp_suppl_file_df['txcode_2'],\n",
    "                              hcp_suppl_file_df['txcode_3']), ' '))))\n",
    "hcp_suppl_file_df = hcp_suppl_file_df.withColumn(\n",
    "    \"middle_name\", F.concat_ws(' ',\n",
    "                F.array_distinct(F.split(concat_ws(\" \", hcp_suppl_file_df['m_name'],\n",
    "                              hcp_suppl_file_df['ma_name'],\n",
    "                              substring('m_name', 0, 1),\n",
    "                              substring('ma_name', 0, 1),\n",
    "                       ), ' '))))\n",
    "hcp_suppl_file_df.select(\"ppid\", \"f_name\", \"l_name\",\n",
    "                          \"address\", \"state_city\", \"taxonomies\", \"middle_name\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format zip into 5 digit zip and 4 codes\n",
    "def formatzip(zip_str):\n",
    "    if zip_str is None:\n",
    "        return ''\n",
    "    l = len(zip_str)\n",
    "    if l==5:\n",
    "        return zip_str\n",
    "    else:\n",
    "        zip_str = '0' * (9-l) + zip_str\n",
    "        zip_str_lst = [(zip_str[i:i+5]) for i in range(0, l, 5)]\n",
    "        zip_str = \" \".join(zip_str_lst)\n",
    "    return zip_str\n",
    "\n",
    "# creating unique combinations of combined columns\n",
    "def concat_uniq(X):\n",
    "    return F.concat_ws(' ', F.array_distinct(F.split(X, ' ')))\n",
    "\n",
    "#NPI file\n",
    "npi_file_df = spark.read.options(header='True').csv(BASE_DIR + 'npidata_pfile_20050523-20211212.csv')\n",
    "\n",
    "npi_file_df = npi_file_df.withColumnRenamed(\"Provider First Name\", \"f_name\")\\\n",
    "                         .withColumnRenamed(\"Provider Last Name (Legal Name)\", \"l_name\")\\\n",
    "                         .withColumnRenamed(\"Provider Middle Name\", \"m_name\")\\\n",
    "                         .withColumnRenamed(\"Provider First Line Business Practice Location Address\", \"adr_ln_1\")\\\n",
    "                         .withColumnRenamed(\"Provider Second Line Business Practice Location Address\", \"adr_ln_2\")\\\n",
    "                         .withColumnRenamed(\"Provider Business Practice Location Address City Name\", \"city\")\\\n",
    "                         .withColumnRenamed(\"Provider Business Practice Location Address State Name\", \"state\")\\\n",
    "                         .withColumnRenamed(\"Provider Business Practice Location Address Postal Code\", \"zip\")\\\n",
    "                         .withColumnRenamed(\"Healthcare Provider Taxonomy Code_1\", \"txcode_1\")\\\n",
    "                         .withColumnRenamed(\"Healthcare Provider Taxonomy Code_2\", \"txcode_2\")\\\n",
    "                         .withColumnRenamed(\"Healthcare Provider Taxonomy Code_3\", \"txcode_3\")\n",
    "\n",
    "formatzip_udf = udf(formatzip, StringType())\n",
    "npi_file_df = npi_file_df.withColumn(\"zip\", formatzip_udf(npi_file_df['zip']))\n",
    "\n",
    "# filter only physicians\n",
    "npi_file_df = npi_file_df.filter(npi_file_df['Entity Type Code'] == 1) \n",
    "\n",
    "\n",
    "npi_file_df = npi_file_df.groupBy(\"NPI\",\"f_name\",\"l_name\")\\\n",
    "    .agg(concat_uniq(F.concat_ws(' ',F.collect_list('adr_ln_1'), F.collect_list('adr_ln_2'))).alias('address'),\n",
    "        concat_uniq(F.concat_ws(' ',F.collect_list('state'), F.collect_list('city'), F.collect_list('zip'))).alias('state_city'),\n",
    "        concat_uniq(F.concat_ws(' ',F.collect_list('txcode_1'),F.collect_list('txcode_2'),F.collect_list('txcode_3'))).alias('taxonomies'),\n",
    "        concat_uniq(F.concat_ws(' ',F.collect_list('m_name'), F.collect_list(substring('m_name', 0, 1)))).alias('middle_name')\n",
    "    )\n",
    "\n",
    "npi_file_df.select(\"NPI\", \"f_name\", \"l_name\",\"address\", \"state_city\", \"taxonomies\", \"middle_name\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort hcp_suppl_file_df by fname and lname\n",
    "hcp_suppl_file =hcp_suppl_file_df.sort('f_name', 'l_name').select(\"ppid\", \"f_name\", \"l_name\",\n",
    "                                             \"address\", \"state_city\", \"taxonomies\", \"middle_name\")\n",
    "hcp_suppl_file.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort npi_file_df by fname and lname\n",
    "npi_file = npi_file_df.sort('f_name', 'l_name').select(\"NPI\", \"f_name\", \"l_name\",\"address\", \"state_city\", \"taxonomies\", \"middle_name\")\n",
    "npi_file.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write sorted dataframes to csvs\n",
    "\n",
    "hcp_suppl_file.coalesce(1).write.option(\"header\",\"true\").csv(BASE_DIR + \"data_processing/combined_out/hcp_suppl\",sep=',')\n",
    "npi_file.coalesce(1).write.option(\"header\",\"true\").csv(BASE_DIR + \"data_processing/combined_out/hcp_npi\",sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
